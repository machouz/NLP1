<p><link href="../swiss.css" rel=stylesheet></link></p>

<p><a href="../index.html">nlp 2019 course</a></p>

<h1>Assignment 1: Sequence Tagging</h1>

<p>In this assignment you will implement a sequence tagger using the Viterbi algorithm.</p>

<p>You will implement 2 versions of the tagger:</p>

<ol>
<li><p>An HMM tagger.</p></li>
<li><p>A MEMM tagger.</p></li>
</ol>

<p>You will evaluate your taggers on two datatsets.</p>

<p>In addition to the code, you should also submit a short writeup, see below.</p>

<p>At the time of the assignment is published, we did not cover the material for
all of the parts, so the submission date is somewhat far into the future. But you can definitely start now, and advised to do so: things take time.</p>

<h1>Software</h1>

<p>You can code either in Python or in Java. You are <strong>highly encouraged</strong> to use
Python.</p>

<h1>Data</h1>

<p>The training and test files (tagged corpora) needed for this assignment are available on <a href="https://piazza.com/biu.ac.il/fall2018/89680/resources">piazza</a></p>

<p>We will be using two datasets: one for part-of-speech tagging, based on newswire text, and the other for named-entity recognition based on Twitter data.</p>

<p>The instructions below assume the part-of-speech tagging data, which is the main task. However, as sequence tagging is a generic task, you will also train and test your taggers on the named entities data.</p>

<h2>HMM Tagger</h2>

<p>When implementing the HMM tagger, there are two tasks: (a) computing the MLE estimates q and e, (b) finding the best sequence based on these quantities using the Viterbi algorithm.</p>

<h3>HMM task 1: MLE estimates (20 pts)</h3>

<p>In this part you need to compute the estimates for e and q quantities (see lecture 3) based on the training data.</p>

<p>The input of this part is a tagged corpus.  The output is two files: <code>e.mle</code> will contain the info needed for computing the estimates of e, and <code>q.mle</code> will contain the info needed for computing the estimates of q. The files will be in a human-readable format.</p>

<p>As dicussed in class, the estimates for q should be based on a weighted linear interpolation of p(c|a,b), p(c|b) and p(c) (see slide 20 in class 3).</p>

<p>The estimates for e should be such that they work for words seen in training, as well as words not seen in training.
That is, you are advised to think about good "word signatures" for use with unknown words that may appear at test time.</p>

<p>The format of <code>e.mle</code> and <code>q.mle</code> files should be the following:
Each line represents an event and a count, seperated by a tab.
For example, lines in <code>e.mle</code> could be:</p>

<p><code>dog NN&lt;TAB&gt;345</code></br>
<code>*UNK* NN&lt;TAB&gt;939</code></p>

<p>While lines in <code>q.mle</code> could be:</p>

<p><code>DT JJ NN&lt;TAB&gt;3232</code></br>
<code>DT JJ&lt;TAB&gt;3333</code></p>

<p>(the numbers here are made up. <code>&lt;TAB&gt;</code> represents a TAB character, i.e. <code>"\t"</code>)</p>

<p>Word-signature events in <code>e.mle</code> should start with a <code>^</code> sign, for example:</p>

<p><code>^Aa NN&lt;TAB&gt;354</code></br></p>

<p>This indicates a signature for a word starting with upper-case letter and continuting with lowercase.
Beyond starting with <code>^</code>, you are free to use whatever format you choose for the word signatures.
Lines for actual words should not start with <code>^</code>.</p>

<p>Beyond saving the corpus-based quantities in <code>q.mle</code> and <code>e.mle</code> you code should also contain methods for computing the actual q and e quantities.
For example, a method called <code>getQ(t1,t2,t3)</code> (or another name) will be used to compute the quantity <code>q(t3|t1,t2)</code> based on the different counts in the <code>q.mle</code> file.
You will use them in the next sections when implementing the actual tagging algorithm.</p>

<p><b>What to submit:</b></p>

<p>A program called <code>MLETrain</code> taking 3 commandline parameters, <code>input_file_name</code>, <code>q.mle</code>, <code>e.mle</code></p>

<p>The program will go over the training data in <code>input_file_name</code>, compute the needed quantities, and write them to the filenames supplied for <code>q.mle</code> and <code>e.mle</code>.</p>

<p>If you work in Java, you program should be run as:</p>

<p><code>
java -jar MLETrain.jar input_file_name q_mle_filename e_mle_filename
</code></p>

<p>If you work in Python, you program should be run as:</p>

<p><code>
python MLETrain.py input_file_name q_mle_filename e_mle_filename
</code></p>

<p>Note: If this part takes more than a few seconds to run, you are doing something very wrong.</p>

<h3>HMM task 2: Greedy decoding (5 pts)</h3>

<p>Implement a greedy tagger. You will assign scores for each position based on the q and e quantities, using the greedy algorithm presented in class.</p>

<p>See "what to submit" instruction in task 3 below.</p>

<h3>HMM task 3: Viterbi decoding (20 pts)</h3>

<p>Implement the viterbi algorithm.  In this part, your viterbi algorithm will use scores based on the e and q quantities from the previous task, but it is advisable (thouth not required) to write it in a way that will make it easy to re-use the vietrbi code to in the MEMM part, by switching to the scores derived form a log-linear model (that is, the viterbi code could use a <code>getScore(word,tag,prev_tag,prev_prev_tag)</code> function that can be implemented in different ways, one of them is based on <code>q</code> and <code>e</code>).</p>

<p>You need to implement the viterbi algorithm for Trigram tagging.
The straight-forward version of the algorithm can be a bit slow, it is better if you add some pruning to not allow some tags at some positions, by restricting the tags certain words can take, or by considering impossible tag sequences. <strong>You will lose 5 points if you do not do so.</strong></p>

<p><b>What to submit:</b></p>

<p>A program called <code>GreedyTag</code> (implementing greedy tagging) taking 5 commandline parameters, <code>input_file_name</code>, <code>q.mle</code>, <code>e.mle</code>, <code>out_file_name</code>, <code>extra_file_name</code>.</p>

<p>A program called <code>HMMTag</code> (implementing viterbi tagging) taking 5 commandline parameters, <code>input_file_name</code>, <code>q.mle</code>, <code>e.mle</code>, <code>out_file_name</code>, <code>extra_file_name</code>.</p>

<p><code>q.mle</code> and <code>e.mle</code> are the names of the files produced by your MLETrain from above.
<code>input_file_name</code> is the name of an input file. The input is one sentence per line, and the tokens are separated by whitepsace.
<code>out_file_name</code> is the name of the output file.  The format here should be the same as the input file for <code>MLETrain</code>.
<code>extra_file_name</code> this is a name of a file which you are free to use to read stuff from, or to ignore.</p>

<p>The purpose of the <code>extra_file_name</code> is to allow for extra information, if you need them for the pruning of the tags, or for setting the lambda values for the interpolation. If you use the extra_file in your code, you need to submit it also together with your code.</p>

<p>The program will tag each sentence in the <code>input_file_name</code> using the tagging algorithm and the MLE estimates, and output the result to <code>out_file_name</code>.</p>

<p>If you work in Java, you programs should be run as:</p>

<p><code>java -jar GreedyTag.jar input_file_name q_mle_filename e_mle_filename output_file_name extra_file_name</code></p>

<p><code>java -jar HMMTag.jar input_file_name q_mle_filename e_mle_filename output_file_name extra_file_name</code></p>

<p>If you work in Python, you programs should be run as:</p>

<p><code>python GreedyTag.py input_file_name q_mle_filename e_mle_filename output_file_name extra_file_name</code></p>

<p><code>python HMMTag.py input_file_name q_mle_filename e_mle_filename output_file_name extra_file_name</code></p>

<p><b> Your tagger should achieve a test-set accuracy of at leat 95\% on the provided POS-tagging dataset. </b></p>

<p><br/>
<br/>
<b> We should be able to train and test your tagger on new files which we provide. We may use a different tagset.</b></p>

<h2>MEMM Tagger</h2>

<p>Like the HMM tagger, the MEMM tagger also has two parts: training and prediction.</p>

<p>Like in the HMM, we will put these in different programs. For the prediction program, you
can likely re-use large parts of the code you used for the HMM tagger.</p>

<h3>MEMM task 1: Model Training (20 pts)</h3>

<p>In this part you will write code that will extract training data from the the training corpus in a format that a log-linear trainer can understand, and then train a model.</p>

<p>This will be a three-stage process.</p>

<ol>
<li><p>Feature Extraction: Read in the train corpus, and produce a file of feature vectors, where each line looks like: </p>

<p><code>label feat1 feat2 ... featn</code> </p>

<p>where all the items are stings. For example, if the features are the word form, the 3-letter suffix, and the previous tag, you could have lines such as </p>

<p><code>NN form=walking suff=ing pt=DT</code>,</p>

<p>indicating a case where the gold label is <code>NN</code>, the word is <code>walking</code>, its suffix is <code>ing</code> and the previous tag is <code>DT</code>.</p></li>
<li><p>Feature Conversion: Read in the features file produced in stage (1), and produce a file in the format the log-linear trainer can read. We will use the format of the liblinear solver (which is a widely used format). Here, output lines could look like: </p>

<p><code>3 125:1 1034:1 2098:1</code></p>

<p>This means we have label number <code>3</code>, and features numbers <code>125</code>, <code>1034</code> and <code>2098</code> each appearing once in this example.  You will also want to save the feature-string-to-numbers mapping, in a file that looks like, e.g.:</p>

<p><code>
pt=DT 125
</code></p>

<p><code>
suff=ing 1034
</code></p>

<p><code>
form=walking 2098
</code></p></li>
<li><p>Model Training: train a model based on the training file produced in (2), and produce a trained model file.</p>

<p>Here, you could use any solver you want. We recommend the <code>liblinear</code> solver. Some hints on using it are available <a href="http://www.cs.biu.ac.il./~89-680/liblinear/using_liblinear.html">here</a>.
While liblinear is convenient, you can use other solvers if you want, as long as they solve a multi-class log-linear model, and as long as you can use it for prediction in the next task.  This classification model goes by various names, including <code>multinomial logistic regression</code>, <code>multiclass logistic regression</code>, <code>maximum entropy</code>, <code>maxent</code>.</p></li>
</ol>

<p>Another popular solver is available in the <a href="http://scikit-learn.org/stable/">scikit-learn</a> python package (also called <code>sklearn</code>). The sklearn package contains many machine learning algorithms, and is worth knowing.
    If you use <code>scikit-learn</code>, you can read data in the liblinear format using <a href="http://scikit-learn.org/stable/datasets/index.html#datasets-in-svmlight-libsvm-format">this loader function</a>.</p>

<pre><code>Some other solvers include [vowpal wabbit](https://github.com/JohnLangford/vowpal_wabbit/wiki) (which is very very fast and can handle very large datasets) and Weka (which used to be popular but is somewhat old).
</code></pre>

<p>Note: sometimes an implementation will mix (1) and (2), or (1), (2) and (3) in the same program. We require you to separate the stages into different programs. Such an architecture is quite common, and it also helps to highlight the different stages of the training process, and make it more convenient to debug them.</p>

<p>To be precise, you need create and submit the following programs:</p>

<ol>
<li><code>ExtractFeatures corpus_file features_file</code></li>
<li><code>ConvertFeatures features_file feature_vecs_file feature_map_file</code></li>
<li><code>TrainSolver feature_vecs_file model_file</code></li>
</ol>

<p>The first parameter to each program is the name of an input file, and the rest are names of output files.
The <code>feature_vecs_file</code> will be in the liblinear format if you use either <code>liblinear</code> of <code>sklearn</code>, or in a different format if you use a different solver. 
The file <code>feature_map_file</code> will contain a mapping from strings to feature ids.</p>

<p>If you implement in Python, we should be able to run your code as <code>python ProgName.py arg1 arg2...</code>. If you use Java, we should be able to run your code as <code>java -jar ProgName.jar arg1 arg2 ...</code>.</p>

<p>If the implementation of <code>TrainSolver</code> is simply using the liblinear solver from the commandline, you can include a <code>.sh</code> file with the commandline needed to run the liblinear solver, which should be run as <code>bash TrainSolver.sh feature_vecs_file model_file</code>. (Inside a bash script, <code>$1</code> stands for the first commandline argument, <code>$2</code> for the second, etc).</p>

<p><strong>Implement the features from table 1 in the MEMM paper (see link in the course website)</strong></p>

<p><b>What to submit:</b></p>

<p>Submit the code for the programs in stages (1), (2), and (3). 
You also need to submit files named <code>features_file_partial</code>, <code>feature_vecs_partial</code>, <code>model_file</code> and <code>feature_map</code>.</p>

<p><code>model_file</code> and <code>feature_map</code> are your trained model and your <code>feature_map_file</code>. The two <code>_partial</code> files are the first 100 and last 100 lines in the <code>features_file</code> and <code>feature_vecs_file</code>.</p>

<p>Assuming your <code>features_file</code> is called <code>memm-features</code>, you can create the partial file on a unix commandline using:</p>

<p><code>cat memm-features | head -100 &gt; features_file_partial</code></p>

<p><code>cat memm-features | tail -100 &gt;&gt; features_file_partial</code></p>

<p>You can verify the number of lines using:</p>

<p><code>cat features_file_partial | wc -l</code></p>

<h3>MEMM task 2: Greedy decoding (5 pts)</h3>

<p>Implement a greedy tagger. You will assign scores for each position based on the max-ent model, in a greedy fashion.</p>

<p>See "what to submit" instruction in task 3 below.</p>

<h3>MEMM task 3: Viterbi Tagger (15 pts)</h3>

<p>Use the model you trained in the previous task inside your vieterbi decoder.</p>

<p><b>What to submit:</b></p>

<p>A program called <code>GreedyMaxEntTag</code> taking 4 commadnline parameters, <code>input_file_name</code>, <code>modelname</code>, <code>feature_map_file</code>, <code>out_file_name</code>.</p>

<p>A program called <code>MEMMTag</code> taking 4 commadnline parameters, <code>input_file_name</code>, <code>modelname</code>, <code>feature_map_file</code>, <code>out_file_name</code>.</p>

<ul>
<li><code>input_file_name</code> is the name of an input file. The input is one sentence per line, and the tokens are separated by whitepsace.</li>
<li><code>modelname</code> is the names of the MaxEnt trained model file.</li>
<li><code>feature_map_file</code> is the name of the features-to-integers file, which can also contain other information if you need it to.</li>
<li><code>out_file_name</code> is the name of the output file.  The format here should be the same as the input file for <code>MLETrain</code>.</li>
</ul>

<p>The purpose of the <code>feature_map_file</code> is to contain the string-to-id mapping, as well as other information which you may need, for example for pruning of the possible tags, like you did in the HMM part.</p>

<p>The programs will tag each sentence in the <code>input_file_name</code> using using either the MaxEnt (logistic-regression) predictions and greedy-decoding, or using the MaxEnt predictions and Viterbi decoding, and output the result to <code>out_file_name</code>.</p>

<p>Note: prediction in this model is likely to be quite a bit slower than in the HMM model. See the bottom of the liblinear instructions for a possible speed-up trick.</p>

<p>If you work in Java, you program should be run as:</p>

<p><code>
java -jar GreedyMaxEntTag.jar input_file_name model_file_name feature_map_file output_file
</code></p>

<p><code>
java -jar MEMMTag.jar input_file_name model_file_name output_file_name feature_map_file output_file
</code></p>

<p>If you work in Python, you program should be run as:</p>

<p><code>
python GreedyMaxEntTag.py input_file_name model_file_name feature_map_file output_file
</code></p>

<p><code>
python MEMMTag.py input_file_name model_file_name feature_map_file output_file
</code></p>

<p><b> Your tagger should achieve a test-set accuracy of at leat 95\% on the provided POS-tagging dataset. </b></p>

<p><strong>Clarification</strong>:
In class (and in the MEMM paper) the features are written as <code>form=dog&amp;label=NN</code>, and we assume a feature for each label, that is, we will also have <code>form=dog&amp;label=DT</code>, <code>form=dog&amp;label=VB</code> etc. That is, the label is part of the feature. In the solvers input, we take a somewhat different approach, and do not include the label as part of the feature. Instead, we write the correct label, and all the label-less features. The solver produces the conjunctions with all the different labels internally.
Over the years, this is something that was very hard for some students to grasp. I made this
<a href="http://www.cs.biu.ac.il/~89-680/weights.pdf">illustration</a> which I hope will help to explain this. The first page is the <code>&amp;ti=T</code> notation, while the second one is what liblinear is expecting.</p>

<h1>The Named Entities Data (15 pts)</h1>

<p>Train and test your taggers also on the <a href="ner.tgz">named entities data</a>.</p>

<p>Note that there are two ways to evaluate the named entities data. One of them is to look at the per-token accuracy, like in POS tagging
(what is your per-token accuracy?).</p>

<p>The NER results are lower than the POS results (how much lower?), look at the data and think why.</p>

<p>Another way is to consider precision, recall and F-measure for correctly identified spans.
(what is your spans F-measure?).</p>

<p>You can perform span-level evaluation using this script: <a href="ner_eval.py">ner_eval.py</a>,
which should be run as: 
<code>
python ner_eval.py gold_file predicted_file
</code></p>

<p>The span-based F scores are lower than the accuracy scores. Why?</p>

<p>Can you improve the MEMM tagger on the NER data?</p>

<p>(maybe these <a href="https://github.com/aritter/twitter_nlp/tree/master/data/annotated/wnut16/lexicon">lexicons</a> can help)</p>

<p><b>What to submit:</b></p>

<ol>
<li><p>An ascii text file named <code>ner.txt</code> containing the per-token accuracy on the ner dev data, and the per-span precision, recall and F-measure on the ner dev data.
You should include numbers based on the HMM and the MEMM taggers. Include also a brief discussion on the "Why"s above, and a brief description of your attempts to improve the MEMM tagger for the NER data.</p></li>
<li><p>Files named <code>ner.memm.pred</code> and <code>ner.hmm.pred</code>, containing the predictions of your HMM and your MEMM models on the <code>test.blind</code> file.</p></li>
</ol>

<h1>Misc</h1>

<p>You can write your code in either Python or Java (or both). You can even mix-and-match, for example, writing the training code in python (for ease of writing) and the viterbi tagger in Java (for efficiency). We recommend using only Python. Use python 2.7 or 3.x. You can use packages such as <code>sklearn</code> or <code>numpy</code>.</p>

<p>The code should be able to run from the commandline, without using Eclipse or any other IDE.</p>

<p>For Java, please submit both the .java source and the compiled .class files.</p>

<p>The code for all assignments should be submitted in a single <code>.zip</code> or <code>.tar.gz</code> file.</p>

<p>The different tasks should be in different directories, named: <code>hmm1 (task 1)</code>, <code>hmm2 (tasks 2 and 3)</code>, <code>memm1</code> (task 1), <code>memm2</code> (tasks 2 and 3), <code>ner</code>.</p>

<p>The writeup should be in the top folder, in a <code>pdf</code> file called <code>writeup.pdf</code>.</p>

<p>You code should run from the commandline on a unix system according to the instruction provided in each section,
assuming you are in the correct folder. </p>

<p>For task Memm-1, please provide instructions on how to run your code, in a <code>README</code> file in the <code>memm1</code> folder.</p>

<p>The code should be able to run directly from within the extracted directories, without needing to copy files between directories.</p>

<p>If your code cannot be run, you will receive a grade of 0.</p>

<h1>Writeup</h1>

<p>Your writeup should include the following:</p>

<ol>
<li>Describe how you handled unknown words in hmm1.</li>
<li>Describe your pruning strategy in the viterbi hmm.</li>
<li>Report your test scores when running the each tagger (hmm-greedy, hmm-viterbi, maxent-greedy, memm-viterbi) on each dataset. For the NER dataset, report token accuracy accuracy, as well as span precision, recall and F1.</li>
<li>Is there a difference in behavior between the hmm and maxent taggers? discuss.</li>
<li>Is there a difference in behavior between the datasets? discuss.</li>
<li>What will you change in the hmm tagger to improve accuracy on the named entities data?</li>
<li>What will you change in the memm tagger to improve accuracy on the named entities data, on top of what you already did?</li>
<li>Why are span scores lower than accuracy scores?</li>
</ol>
